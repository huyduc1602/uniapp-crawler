name: Deploy Uniapp Crawler

on:
  # Run on push to main or master branch
  push:
    branches:
      - main
      - master
  
  # Run on manual trigger
  workflow_dispatch:
    inputs:
      force_crawl:
        description: 'Force crawl even if data already exists'
        required: false
        default: false
        type: boolean
  
  # Run on schedule (cronjob) - Every Monday at 01:00 UTC
  schedule:
    - cron: '0 1 * * 1'

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    # Fetch the repository content
    - name: Checkout code
      uses: actions/checkout@v3

    # Set up Docker environment for building images
    - name: Set up Docker
      uses: docker/setup-buildx-action@v2

    # Authenticate with Docker Hub using repository secrets
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    # Add the translation API key to the environment file
    - name: Add DEEPL_API_KEY to environment
      run: echo "DEEPL_API_KEY=${{ secrets.DEEPL_API_KEY }}" >> .env
      
    # Add crawling limits to environment file
    - name: Configure crawling limits
      run: |
        echo "MAX_DEPTH=3" >> .env
        echo "MAX_PAGES=50" >> .env

    # Install Python dependencies required by the crawler and translator
    - name: Install Python dependencies
      run: |
        pip install -r app/requirements.txt

    # Ensure docker-compose is available for orchestrating containers
    - name: Install docker-compose
      run: |
        sudo apt-get update
        sudo apt-get install -y docker-compose

    # Run only crawler if data/zh is empty or force_crawl flag is set
    - name: Check if crawling is needed
      id: check_crawling
      run: |
        if [ -z "$(ls -A data/zh 2>/dev/null)" ] || [ "${{ github.event.inputs.force_crawl }}" == "true" ] || [ "${{ github.event_name }}" == "schedule" ]; then
          echo "need_crawling=true" >> $GITHUB_OUTPUT
        else
          echo "need_crawling=false" >> $GITHUB_OUTPUT
        fi

    # Run only crawler service if needed
    - name: Run crawler (if needed)
      if: steps.check_crawling.outputs.need_crawling == 'true'
      run: |
        echo "Running crawler..."
        docker-compose up --build crawler
      
    # Always run translator service
    - name: Run translator
      run: |
        echo "Running translator..."
        docker-compose up --build translator

    # Copy translated files to the docs folder for GitHub Pages
    - name: Copy translated documents to docs folder
      run: |
        mkdir -p docs
        cp -r data/vi/* docs/ || echo "No files to copy"
        ls -la docs/

    # Stop and remove Docker containers
    - name: Clean up
      run: |
        docker-compose down

    # Commit and push the generated docs to the repository
    - name: Commit updates to docs
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add docs/
        git commit -m "Update docs with latest translations ($(date +'%Y-%m-%d'))" || echo "No changes to commit"
        git push